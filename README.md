# Evaluating-Classification-Models

* *Precision*: Of the predictions the model made for this class, what proportion were correct?
* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?
* *F1-Score*: An average metric that takes both precision and recall into account.
* *Support*: How many instances of this class are there in the test dataset?
*  *ROC Curve*: The area under the curve (*AUC*) is a value between 0 and 1 that quantifies the overall performance of the model.
